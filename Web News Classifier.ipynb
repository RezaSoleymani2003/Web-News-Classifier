{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#Random\n",
    "import random\n",
    "\n",
    "#CSV\n",
    "import csv  \n",
    "\n",
    "#OS\n",
    "import os\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "#Seyed\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import trafilatura\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCsv(\n",
    "    type,\n",
    "    source,\n",
    "    url,\n",
    "    id,\n",
    "    topic,\n",
    "    author,\n",
    "    timeRelease,\n",
    "    vote,\n",
    "    commentsCount,\n",
    "    language,\n",
    "    token,\n",
    "    predicte,\n",
    "):\n",
    "    columnTopics = [\n",
    "        \"Type\",\n",
    "        \"Source\",\n",
    "        \"URL\",\n",
    "        \"ID\",\n",
    "        \"Topic\",\n",
    "        \"Author\",\n",
    "        \"TimeRelease\",\n",
    "        \"Vote\",\n",
    "        \"CommentsCount\",\n",
    "        \"Language\",\n",
    "        \"Token\",\n",
    "        \"Predicte\",\n",
    "    ]\n",
    "    row = [\n",
    "        type,\n",
    "        source,\n",
    "        url,\n",
    "        id,\n",
    "        topic,\n",
    "        author,\n",
    "        timeRelease,\n",
    "        vote,\n",
    "        commentsCount,\n",
    "        language,\n",
    "        token,\n",
    "        predicte,\n",
    "    ]\n",
    "    \n",
    "    fileName = \"RecordsNewsReddit.csv\"\n",
    "    file_exists = os.path.exists(fileName)\n",
    "    \n",
    "    with open(fileName, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(columnTopics)\n",
    "        \n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Save Data\n",
    "saveCsv(\n",
    "    type=\"Reddit\",\n",
    "    source=\"r/news\",\n",
    "    url=\"https://reddit.com/r/news/xyz\",\n",
    "    id=\"abc123\",\n",
    "    topic=\"Breaking News\",\n",
    "    author=\"user42\",\n",
    "    timeRelease=\"2025-09-08 12:00\",\n",
    "    vote=420,\n",
    "    commentsCount=69,\n",
    "    language=\"en\",\n",
    "    token=\"breaking,news\",\n",
    "    predicte=\"Politics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init -> (userAgent)(no need)\n",
    "→ (login)(Optional)\n",
    "→ loop { \n",
    "     rateLimiter\n",
    "     randomDelay\n",
    "     fetch/searchSubReddit\n",
    "     cleanData(deduplication + canonicalization)\n",
    "     saveData(incremental)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "def init():\n",
    "    try:\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15.6; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:142.0) Gecko/20100101 Firefox/142.0\"\n",
    "        ]\n",
    "        random_agent = random.choice(user_agents)\n",
    "\n",
    "        options = Options()\n",
    "        options.set_preference(\"general.useragent.override\", random_agent)\n",
    "\n",
    "        service = Service(\"/usr/bin/geckodriver\")\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        \n",
    "        print(f\"[INFO] Using User-Agent: {random_agent}\")\n",
    "        return driver\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(\"WebDriver error happened:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomDelay\n",
    "def randomDelay():\n",
    "    time.sleep(random.uniform(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(type, source, post_id, title, author, votes, comments, time_release, url):\n",
    "    \"\"\"Clean, deduplicate, canonicalize, and save post.\"\"\"\n",
    "    fileName = \"RecordsNewsReddit.csv\"\n",
    "    \n",
    "    # Canonicalize: strip strings\n",
    "    post_id = post_id.strip() if post_id else \"N/A\"\n",
    "    title = title.strip() if title else \"N/A\"\n",
    "    author = author.strip() if author else \"N/A\"\n",
    "    url = url.strip() if url else \"N/A\"\n",
    "    \n",
    "    # Convert votes/comments to int if possible\n",
    "    try:\n",
    "        votes = int(votes)\n",
    "    except:\n",
    "        votes = 0\n",
    "    try:\n",
    "        comments = int(comments)\n",
    "    except:\n",
    "        comments = 0\n",
    "    \n",
    "    # Convert time_release to standard format\n",
    "    try:\n",
    "        if time_release != \"N/A\":\n",
    "            dt = datetime.fromisoformat(time_release.replace(\"Z\", \"+00:00\"))\n",
    "            time_release = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            time_release = \"N/A\"\n",
    "    except:\n",
    "        time_release = \"N/A\"\n",
    "    \n",
    "    # Deduplication: check existing CSV\n",
    "    existing_ids = set()\n",
    "    if os.path.isfile(fileName):\n",
    "        with open(fileName, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_ids.add(row[\"ID\"])\n",
    "    \n",
    "    if post_id in existing_ids:\n",
    "        print(f\"Duplicate found: {post_id}, skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Save to CSV\n",
    "    saveCsv(\n",
    "        type=type,\n",
    "        source=source,\n",
    "        url=url,\n",
    "        id=post_id,\n",
    "        topic=title,\n",
    "        author=author,\n",
    "        timeRelease=time_release,\n",
    "        vote=votes,\n",
    "        commentsCount=comments,\n",
    "        language=\"N/A\",\n",
    "        token=\"N/A\",\n",
    "        predicte=\"N/A\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xpath\n",
    "\n",
    "/html/body/shreddit-app/div[3]/div/div[2]/main/div[2]/shreddit-feed/article[1]/shreddit-post/a\n",
    "\n",
    "/html/body/shreddit-app/div[3]/div/div[2]/main/div[2]/shreddit-feed/article[2]/shreddit-post/a\n",
    "\n",
    "=====================\n",
    "\n",
    "CSS Selector:\n",
    "#t3_1nc1j6c > a:nth-child(1)\n",
    "#t3_1n43yfc > a:nth-child(1)\n",
    "\n",
    "=====================\n",
    "\n",
    "[json for reddit](https://www.reddit.com/r/news/.json)\n",
    "\n",
    "-----------------------\n",
    "(https://www.reddit.com/r/news/comments/1mvlhxn/texas_cant_require_the_ten_commandments_in_every/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDataSubReddit(driver, type, source, total_posts=50, posts_before_cooldown=30, cooldown_time=60):\n",
    "    scraped = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Track last time we saved a post\n",
    "    last_scrape_time = time.time()\n",
    "\n",
    "    while scraped < total_posts:\n",
    "        # Check if 2 minutes passed without scraping new posts\n",
    "        if time.time() - last_scrape_time > 180:\n",
    "            print(\"No new posts saved for 3 minutes. Stopping...\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Wait until at least one post is loaded\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"shreddit-post\"))\n",
    "            )\n",
    "            posts = driver.find_elements(By.CSS_SELECTOR, \"shreddit-post\")\n",
    "\n",
    "            # If next post is not loaded, scroll a bit and wait\n",
    "            if scraped >= len(posts):\n",
    "                driver.execute_script(\"window.scrollBy(0, 800);\")  # small scroll\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "                continue\n",
    "\n",
    "            post = posts[scraped]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No posts loaded yet or error: {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "        # Extract attributes safely\n",
    "        post_id = post.get_attribute(\"id\") or \"N/A\"\n",
    "        permalink = post.get_attribute(\"permalink\") or \"N/A\"\n",
    "        title = post.get_attribute(\"post-title\") or \"N/A\"\n",
    "        author = post.get_attribute(\"author\") or \"N/A\"\n",
    "        votes = post.get_attribute(\"score\") or \"0\"\n",
    "        comments = post.get_attribute(\"comment-count\") or \"0\"\n",
    "        time_release = post.get_attribute(\"created-timestamp\") or \"N/A\"\n",
    "        url = post.get_attribute(\"content-href\") or \"N/A\"\n",
    "\n",
    "        # Clean, deduplicate, and save\n",
    "        cleanData(type, source, post_id, title, author, votes, comments, time_release, url)\n",
    "\n",
    "        scraped += 1\n",
    "        last_scrape_time = time.time()  # reset timer since we got a new post\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        # Cooldown after N posts\n",
    "        if scraped % posts_before_cooldown == 0:\n",
    "            print(f\"Cooldown: {scraped} posts scraped. Waiting {cooldown_time} sec...\")\n",
    "            time.sleep(cooldown_time)\n",
    "\n",
    "        # Scroll if at bottom of loaded content\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        last_height = new_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subReddits List\n",
    "subRedditsNews = [\n",
    "    \"r/news/\",\n",
    "    \"r/worldnews/\",\n",
    "    \"r/realbbcnews/\",\n",
    "    \"r/world24x7hr/\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch/searchSubReddit\n",
    "def searchRedditNews(subRedditsNews, total_posts_per_subreddit=1000):\n",
    "    baseURLReddit = \"https://www.reddit.com/\"\n",
    "    driver = init()\n",
    "    for subreddit in subRedditsNews:   \n",
    "        full_url = baseURLReddit + subreddit     \n",
    "        driver.get(full_url) \n",
    "        clean_url = driver.execute_script(\"return window.location.origin + window.location.pathname\")\n",
    "        driver.get(clean_url)        \n",
    "        type = \"reddit\"\n",
    "        source = subreddit\n",
    "        randomDelay()       \n",
    "        extractDataSubReddit(driver, type, source, total_posts=total_posts_per_subreddit)\n",
    "        time.sleep(10)\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchRedditNews(subRedditsNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_seeds = [\n",
    "    \"https://www.theguardian.com/world\", # 54\n",
    "    \"https://www.aljazeera.com/news/\", # 19\n",
    "    \"https://www.nytimes.com/section/world\",# forbidden url\n",
    "    \"https://www.nbcnews.com/world\",# keyword: world , 22\n",
    "]\n",
    "\n",
    "visited = set()\n",
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html, url):\n",
    "    \"\"\"Try to extract main article text using trafilatura, fallback to <p> tags.\"\"\"\n",
    "    text = trafilatura.extract(html, url=url)\n",
    "    if text:\n",
    "        return text\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
    "\n",
    "def crawl(url, index, depth=1):\n",
    "    if depth == 0 or url in visited:\n",
    "        return\n",
    "    \n",
    "    if True:\n",
    "        print(f\"Crawling: {url}\")\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return\n",
    "        \n",
    "        html = response.text\n",
    "        \n",
    "        text = extract_text(html, url)\n",
    "        \n",
    "        if text:\n",
    "            if \"2025\" in url:\n",
    "                articles.append({\"url\": url, \"id\": 0, \"title\": 0, \"writer\": 0,  \"raw_text\": text})\n",
    "                print(f\"Saved article ({len(text)} chars)\")\n",
    "        else:\n",
    "            print(f\"Not an article, but checking links...\")\n",
    "        \n",
    "        # Parse links\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            new_url = urljoin(url, link[\"href\"])\n",
    "            \n",
    "            # Filter: must be within same domain as seed\n",
    "            if new_url not in visited and url.split(\"/\")[2] in new_url:\n",
    "                crawl(new_url, index, depth - 1)\n",
    "\n",
    "\n",
    "crawl(news_seeds[0], 0, depth=2)\n",
    "\n",
    "print(f\"Collected {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[10]['raw_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(articles):\n",
    "    for article in articles:\n",
    "        text = article['raw_text']\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        # Remove special characters (keep only words and numbers)\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "        article['normalize_text'] = text\n",
    "\n",
    "# normalize(articles)\n",
    "# print(articles[10]['normalize_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(articles):\n",
    "    for article in articles:\n",
    "        tokens = nltk.word_tokenize(article['normalize_text'])\n",
    "    \n",
    "        # Remove stopwords\n",
    "        article[\"tokens\"] = [t for t in tokens if t not in stop_words] \n",
    "\n",
    "# tokenize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [stemmer.stem(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# stem(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [lemmatizer.lemmatize(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# lemmatize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(articles):\n",
    "    normalize(articles)\n",
    "    tokenize(articles)\n",
    "    stem(articles)\n",
    "    # lemmatize(articles)\n",
    "    \n",
    "preprocess(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 200000\n",
    "\n",
    "dataset_articles = []\n",
    "for fileid in reuters.fileids()[:dataset_size]:\n",
    "    text = reuters.raw(fileid)\n",
    "    cats = reuters.categories(fileid)  # some docs have multiple categories\n",
    "    if cats:\n",
    "        dataset_articles.append({\"raw_text\": text, \"label\": cats[0]})\n",
    "\n",
    "dataset_articles[10][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(dataset_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X_reuters = vectorizer.fit_transform([\" \".join(dataset_article[\"tokens\"]) for dataset_article in dataset_articles])\n",
    "X_crawled = vectorizer.transform([\" \".join(article[\"tokens\"]) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vocab = {}\n",
    "labels = [dataset_article[\"label\"] for dataset_article in dataset_articles]\n",
    "labels = list(set(labels))  # unique labels\n",
    "\n",
    "for label in labels:\n",
    "    # find indices of docs belonging to this label\n",
    "    idx = [i for i, art in enumerate(dataset_articles) if art[\"label\"] == label]\n",
    "    \n",
    "    if not idx:  # skip empty categories\n",
    "        continue\n",
    "    \n",
    "    # average TF-IDF weights for this category\n",
    "    avg_weights = np.asarray(X_reuters[idx].mean(axis=0)).ravel()\n",
    "    \n",
    "    # get indices of top 50 highest-weighted terms\n",
    "    top_idx = avg_weights.argsort()[::-1][:50]\n",
    "    \n",
    "    # get actual term strings\n",
    "    terms = vectorizer.get_feature_names_out()[top_idx]\n",
    "    \n",
    "    # get their corresponding weights\n",
    "    weights = avg_weights[top_idx]\n",
    "    \n",
    "    # save into dictionary\n",
    "    category_vocab[label] = dict(zip(terms, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_article(article_vector):\n",
    "    scores = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    vec_array = article_vector.toarray().ravel()\n",
    "    for label, vocab in category_vocab.items():\n",
    "        score = sum(vec_array[feature_names.tolist().index(term)] * w\n",
    "                    for term, w in vocab.items() if term in feature_names)\n",
    "        scores[label] = score\n",
    "    return scores\n",
    "\n",
    "for article, x in zip(articles, X_crawled):\n",
    "    scores = score_article(x)  # returns a dict of {label: score}\n",
    "    article[\"scores\"] = scores\n",
    "    article[\"predicted\"] = max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[10][\"predicted\"]\n",
    "print(Counter(a[\"predicted\"] for a in articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:5]:\n",
    "    print(article[\"raw_text\"])\n",
    "    print(f\"### predicted class: {article['predicted']}\")\n",
    "    print(\"*\"*20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
