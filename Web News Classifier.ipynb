{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#Random\n",
    "import random\n",
    "\n",
    "#CSV\n",
    "import csv  \n",
    "\n",
    "#OS\n",
    "import os\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "#\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "#Seyed\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import trafilatura\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCsv(\n",
    "    type,\n",
    "    source,\n",
    "    url,\n",
    "    id,\n",
    "    topic,\n",
    "    author,\n",
    "    timeRelease,\n",
    "    vote,\n",
    "    commentsCount,\n",
    "    language,\n",
    "    token,\n",
    "    predicte,\n",
    "    rawText,\n",
    "):\n",
    "    columnTopics = [\n",
    "        \"Type\",\n",
    "        \"Source\",\n",
    "        \"URL\",\n",
    "        \"ID\",\n",
    "        \"Topic\",\n",
    "        \"Author\",\n",
    "        \"TimeRelease\",\n",
    "        \"Vote\",\n",
    "        \"CommentsCount\",\n",
    "        \"Language\",\n",
    "        \"Token\",\n",
    "        \"Predicte\",\n",
    "        \"RawText\",\n",
    "    ]\n",
    "    row = [\n",
    "        type,\n",
    "        source,\n",
    "        url,\n",
    "        id,\n",
    "        topic,\n",
    "        author,\n",
    "        timeRelease,\n",
    "        vote,\n",
    "        commentsCount,\n",
    "        language,\n",
    "        token,\n",
    "        predicte,\n",
    "        rawText\n",
    "    ]\n",
    "    \n",
    "    fileName = \"RecordsNewsReddit.csv\"\n",
    "    file_exists = os.path.exists(fileName)\n",
    "    \n",
    "    with open(fileName, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(columnTopics)\n",
    "        \n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' #Example Save Data\\nsaveCsv(\\n    type=\"Reddit\",\\n    source=\"r/news\",\\n    url=\"https://reddit.com/r/news/xyz\",\\n    id=\"abc123\",\\n    topic=\"Breaking News\",\\n    author=\"user42\",\\n    timeRelease=\"2025-09-08 12:00\",\\n    vote=420,\\n    commentsCount=69,\\n    language=\"en\",\\n    token=\"breaking,news\",\\n    predicte=\"Politics\"\\n) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #Example Save Data\n",
    "saveCsv(\n",
    "    type=\"Reddit\",\n",
    "    source=\"r/news\",\n",
    "    url=\"https://reddit.com/r/news/xyz\",\n",
    "    id=\"abc123\",\n",
    "    topic=\"Breaking News\",\n",
    "    author=\"user42\",\n",
    "    timeRelease=\"2025-09-08 12:00\",\n",
    "    vote=420,\n",
    "    commentsCount=69,\n",
    "    language=\"en\",\n",
    "    token=\"breaking,news\",\n",
    "    predicte=\"Politics\"\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init -> (userAgent)(no need)\n",
    "→ (login)(Optional)\n",
    "→ loop { \n",
    "     rateLimiter\n",
    "     randomDelay\n",
    "     fetch/searchSubReddit\n",
    "     cleanData(deduplication + canonicalization)\n",
    "     saveData(incremental)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "def init():\n",
    "    try:\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 15.6; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64; rv:142.0) Gecko/20100101 Firefox/142.0\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:142.0) Gecko/20100101 Firefox/142.0\"\n",
    "        ]\n",
    "        random_agent = random.choice(user_agents)\n",
    "\n",
    "        options = Options()\n",
    "        options.set_preference(\"general.useragent.override\", random_agent)\n",
    "\n",
    "        service = Service(\"/usr/bin/geckodriver\")\n",
    "        driver = webdriver.Firefox(service=service, options=options)\n",
    "        \n",
    "        print(f\"[INFO] Using User-Agent: {random_agent}\")\n",
    "        return driver\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(\"WebDriver error happened:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomDelay\n",
    "def randomDelay():\n",
    "    time.sleep(random.uniform(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(type, source, post_id, title, author, votes, comments, time_release, url):\n",
    "    \"\"\"Clean, deduplicate, canonicalize, and save post.\"\"\"\n",
    "    fileName = \"RecordsNewsReddit.csv\"\n",
    "    \n",
    "    # Canonicalize: strip strings\n",
    "    post_id = post_id.strip() if post_id else \"N/A\"\n",
    "    title = title.strip() if title else \"N/A\"\n",
    "    author = author.strip() if author else \"N/A\"\n",
    "    url = url.strip() if url else \"N/A\"\n",
    "    \n",
    "    # Convert votes/comments to int if possible\n",
    "    try:\n",
    "        votes = int(votes)\n",
    "    except:\n",
    "        votes = 0\n",
    "    try:\n",
    "        comments = int(comments)\n",
    "    except:\n",
    "        comments = 0\n",
    "    \n",
    "    # Convert time_release to standard format\n",
    "    try:\n",
    "        if time_release != \"N/A\":\n",
    "            dt = datetime.fromisoformat(time_release.replace(\"Z\", \"+00:00\"))\n",
    "            time_release = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            time_release = \"N/A\"\n",
    "    except:\n",
    "        time_release = \"N/A\"\n",
    "    \n",
    "    # Deduplication: check existing CSV\n",
    "    existing_ids = set()\n",
    "    if os.path.isfile(fileName):\n",
    "        with open(fileName, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                existing_ids.add(row[\"ID\"])\n",
    "    \n",
    "    if post_id in existing_ids:\n",
    "        print(f\"Duplicate found: {post_id}, skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Save to CSV\n",
    "    saveCsv(\n",
    "        type=type,\n",
    "        source=source,\n",
    "        url=url,\n",
    "        id=post_id,\n",
    "        topic=title,\n",
    "        author=author,\n",
    "        timeRelease=time_release,\n",
    "        vote=votes,\n",
    "        commentsCount=comments,\n",
    "        language=\"N/A\",\n",
    "        token=\"N/A\",\n",
    "        predicte=\"N/A\",\n",
    "        rawText=\"N/A\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xpath\n",
    "\n",
    "/html/body/shreddit-app/div[3]/div/div[2]/main/div[2]/shreddit-feed/article[1]/shreddit-post/a\n",
    "\n",
    "/html/body/shreddit-app/div[3]/div/div[2]/main/div[2]/shreddit-feed/article[2]/shreddit-post/a\n",
    "\n",
    "=====================\n",
    "\n",
    "CSS Selector:\n",
    "#t3_1nc1j6c > a:nth-child(1)\n",
    "#t3_1n43yfc > a:nth-child(1)\n",
    "\n",
    "=====================\n",
    "\n",
    "[json for reddit](https://www.reddit.com/r/news/.json)\n",
    "\n",
    "-----------------------\n",
    "(https://www.reddit.com/r/news/comments/1mvlhxn/texas_cant_require_the_ten_commandments_in_every/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDataSubReddit(driver, type, source, total_posts=50, posts_before_cooldown=30, cooldown_time=60):\n",
    "    scraped = 0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Track last time we saved a post\n",
    "    last_scrape_time = time.time()\n",
    "\n",
    "    while scraped < total_posts:\n",
    "        # Check if 3 minutes passed without scraping new posts\n",
    "        if time.time() - last_scrape_time > 180:\n",
    "            print(\"No new posts saved for 3 minutes. Stopping...\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Wait until at least one post is loaded\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"shreddit-post\"))\n",
    "            )\n",
    "            posts = driver.find_elements(By.CSS_SELECTOR, \"shreddit-post\")\n",
    "\n",
    "            # If next post is not loaded, scroll a bit and wait\n",
    "            if scraped >= len(posts):\n",
    "                driver.execute_script(\"window.scrollBy(0, 800);\")  # small scroll\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "                continue\n",
    "\n",
    "            post = posts[scraped]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No posts loaded yet or error: {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "\n",
    "        # Extract attributes safely\n",
    "        post_id = post.get_attribute(\"id\") or \"N/A\"\n",
    "        permalink = post.get_attribute(\"permalink\") or \"N/A\"\n",
    "        title = post.get_attribute(\"post-title\") or \"N/A\"\n",
    "        author = post.get_attribute(\"author\") or \"N/A\"\n",
    "        votes = post.get_attribute(\"score\") or \"0\"\n",
    "        comments = post.get_attribute(\"comment-count\") or \"0\"\n",
    "        time_release = post.get_attribute(\"created-timestamp\") or \"N/A\"\n",
    "        url = post.get_attribute(\"content-href\") or \"N/A\"\n",
    "\n",
    "        # Clean, deduplicate, and save\n",
    "        cleanData(type, source, post_id, title, author, votes, comments, time_release, url)\n",
    "\n",
    "        scraped += 1\n",
    "        last_scrape_time = time.time()  # reset timer since we got a new post\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "\n",
    "        # Cooldown after N posts\n",
    "        if scraped % posts_before_cooldown == 0:\n",
    "            print(f\"Cooldown: {scraped} posts scraped. Waiting {cooldown_time} sec...\")\n",
    "            time.sleep(cooldown_time)\n",
    "\n",
    "        # Scroll if at bottom of loaded content\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "            time.sleep(random.uniform(1, 2))\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        last_height = new_height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subReddits List\n",
    "subRedditsNews = [\n",
    "    \"r/news/\",\n",
    "    \"r/worldnews/\",\n",
    "    \"r/realbbcnews/\",\n",
    "    \"r/world24x7hr/\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch/searchSubReddit\n",
    "def searchRedditNews(subRedditsNews, total_posts_per_subreddit=1):\n",
    "    baseURLReddit = \"https://www.reddit.com/\"\n",
    "    driver = init()\n",
    "    for subreddit in subRedditsNews:   \n",
    "        full_url = baseURLReddit + subreddit     \n",
    "        driver.get(full_url) \n",
    "        clean_url = driver.execute_script(\"return window.location.origin + window.location.pathname\")\n",
    "        driver.get(clean_url)        \n",
    "        type = \"reddit\"\n",
    "        source = subreddit\n",
    "        randomDelay()       \n",
    "        extractDataSubReddit(driver, type, source, total_posts=total_posts_per_subreddit)\n",
    "        time.sleep(10)\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using User-Agent: Mozilla/5.0 (X11; Linux i686; rv:142.0) Gecko/20100101 Firefox/142.0\n"
     ]
    }
   ],
   "source": [
    "searchRedditNews(subRedditsNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from each news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text(url):\n",
    "    \"\"\"Fetch raw text from a URL using requests + BeautifulSoup.\"\"\"\n",
    "    # Skip image URLs\n",
    "    if any(x in url for x in [\"i.redd.it\", \"redd.it\", \"reddit\"]):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/116.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {url}, status code: {response.status_code}\")\n",
    "            return \"N/A\"\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        raw_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        if not raw_text:\n",
    "            return \"N/A\"\n",
    "        return raw_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"N/A\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeRawTextFromCSV(csv_file=\"RecordsNewsReddit.csv\"):\n",
    "    \"\"\"Scrape RawText from URLs in a CSV using requests + BeautifulSoup.\"\"\"\n",
    "    if not os.path.isfile(csv_file):\n",
    "        print(f\"CSV file {csv_file} not found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_file, keep_default_na=False)\n",
    "\n",
    "    # Create RawText column if it doesn't exist\n",
    "    if \"RawText\" not in df.columns:\n",
    "        df[\"RawText\"] = \"\"\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if str(row.get(\"Type\", \"\")).lower() != \"reddit\":\n",
    "            continue\n",
    "\n",
    "        raw_text_val = str(row.get(\"RawText\", \"\"))\n",
    "        if raw_text_val.strip() not in [\"\", \"N/A\"]:\n",
    "            # Already scraped\n",
    "            continue\n",
    "\n",
    "        url = row.get(\"URL\", \"\")\n",
    "        if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL: {url}\")\n",
    "            df.at[idx, \"RawText\"] = \"N/A\"\n",
    "            continue\n",
    "\n",
    "        print(f\"Scraping raw text from: {url}\")\n",
    "        raw_text = get_raw_text(url)\n",
    "        df.at[idx, \"RawText\"] = raw_text\n",
    "\n",
    "        # Small delay to avoid overloading servers    \n",
    "        # Save updated CSV\n",
    "        df.to_csv(csv_file, index=False, encoding=\"utf-8\") \n",
    "        print(f\"we are saving {row.get('ID','')}.\")   \n",
    "        time.sleep(1)\n",
    "        \n",
    "    print(f\"CSV updated in-place: {csv_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping raw text from: https://www.kiro7.com/news/local/wa-congressional-candidates-husband-military-veteran-taken-into-ice-custody/AGN6756IA5F7ZHHEQYRSIXWAOM/?outputType=amp\n",
      "we are saving t3_1mz25ee.\n",
      "Scraping raw text from: https://newhampshirebulletin.com/2025/08/14/number-of-canadian-tourists-visiting-new-hampshire-down-30-this-year-officials-say/\n",
      "Failed to fetch https://newhampshirebulletin.com/2025/08/14/number-of-canadian-tourists-visiting-new-hampshire-down-30-this-year-officials-say/, status code: 403\n",
      "we are saving t3_1mrxs29.\n",
      "Scraping raw text from: https://www.france24.com/en/france/20250826-france-to-sue-australian-platform-kick-for-negligence-after-livestream-death\n",
      "Failed to fetch https://www.france24.com/en/france/20250826-france-to-sue-australian-platform-kick-for-negligence-after-livestream-death, status code: 403\n",
      "we are saving t3_1n0mds2.\n",
      "Scraping raw text from: https://www.nytimes.com/2025/08/26/us/politics/doge-social-security-data.html?unlocked_article_code=1.hE8.Zz4X.KRumBN4DjNR1&smid=nytcore-ios-share&referringSource=articleShare\n",
      "Failed to fetch https://www.nytimes.com/2025/08/26/us/politics/doge-social-security-data.html?unlocked_article_code=1.hE8.Zz4X.KRumBN4DjNR1&smid=nytcore-ios-share&referringSource=articleShare, status code: 403\n",
      "we are saving t3_1n0m3mh.\n",
      "Scraping raw text from: https://www.fox13memphis.com/news/2-year-old-girl-shot-during-fight-between-adult-sisters-at-memphis-barbecue-police-say/article_5615868f-fbe7-4313-9d78-ba36e203b215.html\n",
      "Failed to fetch https://www.fox13memphis.com/news/2-year-old-girl-shot-during-fight-between-adult-sisters-at-memphis-barbecue-police-say/article_5615868f-fbe7-4313-9d78-ba36e203b215.html, status code: 451\n",
      "we are saving t3_1mzrswj.\n",
      "Scraping raw text from: https://www.reuters.com/world/americas/canada-remove-many-retaliatory-tariffs-us-goods-says-carney-2025-08-22/\n",
      "Failed to fetch https://www.reuters.com/world/americas/canada-remove-many-retaliatory-tariffs-us-goods-says-carney-2025-08-22/, status code: 401\n",
      "we are saving t3_1mxeire.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mscrapeRawTextFromCSV\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mscrapeRawTextFromCSV\u001b[39m\u001b[34m(csv_file)\u001b[39m\n\u001b[32m     34\u001b[39m     df.to_csv(csv_file, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwe are saving \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow.get(\u001b[33m'\u001b[39m\u001b[33mID\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)   \n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCSV updated in-place: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scrapeRawTextFromCSV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_THREADS = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_seeds = [\n",
    "    \"https://www.theguardian.com/world\", # 54\n",
    "    \"https://www.aljazeera.com/news/\", # 19\n",
    "    \"https://www.nytimes.com/section/world\",# forbidden url\n",
    "    \"https://www.nbcnews.com/world\",# keyword: world , 22\n",
    "]\n",
    "\n",
    "visited = set()\n",
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.theguardian.com/world\n",
      "Crawling: https://www.theguardian.com/world#maincontent\n",
      "Crawling: https://www.theguardian.com/world#navigation\n",
      "Crawling: https://www.theguardian.com/email-newsletters\n",
      "Crawling: https://www.theguardian.com/preference/edition/us\n",
      "Crawling: https://www.theguardian.com/preference/edition/uk\n",
      "Crawling: https://www.theguardian.com/preference/edition/au\n",
      "Crawling: https://www.theguardian.com/preference/edition/eur\n",
      "Crawling: https://www.theguardian.com/preference/edition/int\n",
      "Crawling: https://www.theguardian.com/\n",
      "Crawling: https://www.theguardian.com/commentisfree\n",
      "Crawling: https://www.theguardian.com/sport\n",
      "Crawling: https://www.theguardian.com/culture\n",
      "Crawling: https://www.theguardian.com/lifeandstyle\n",
      "Crawling: https://www.theguardian.com/us-news\n",
      "Crawling: https://www.theguardian.com/us-news/us-politics\n",
      "Crawling: https://www.theguardian.com/environment/climate-crisis\n",
      "Crawling: https://www.theguardian.com/world/middleeast\n",
      "Crawling: https://www.theguardian.com/world/ukraine\n",
      "Crawling: https://www.theguardian.com/us-news/usimmigration\n",
      "Crawling: https://www.theguardian.com/us/soccer\n",
      "Crawling: https://www.theguardian.com/us/business\n",
      "Crawling: https://www.theguardian.com/us/environment\n",
      "Crawling: https://www.theguardian.com/us/technology\n",
      "Crawling: https://www.theguardian.com/science\n",
      "Crawling: https://www.theguardian.com/us/wellness\n",
      "Crawling: https://www.theguardian.com/profile/editorial\n",
      "Crawling: https://www.theguardian.com/index/contributors\n",
      "Crawling: https://www.theguardian.com/tone/letters\n",
      "Crawling: https://www.theguardian.com/type/video+tone/comment\n",
      "Crawling: https://www.theguardian.com/tone/cartoons\n",
      "Crawling: https://www.theguardian.com/sport/nfl\n",
      "Crawling: https://www.theguardian.com/sport/tennis\n",
      "Crawling: https://www.theguardian.com/sport/mlb\n",
      "Crawling: https://www.theguardian.com/football/mls\n",
      "Crawling: https://www.theguardian.com/sport/nba\n",
      "Crawling: https://www.theguardian.com/sport/wnba\n",
      "Crawling: https://www.theguardian.com/sport/nhl\n",
      "Crawling: https://www.theguardian.com/sport/formulaone\n",
      "Crawling: https://www.theguardian.com/sport/golf\n",
      "Crawling: https://www.theguardian.com/film\n",
      "Crawling: https://www.theguardian.com/books\n",
      "Crawling: https://www.theguardian.com/music\n",
      "Crawling: https://www.theguardian.com/artanddesign\n",
      "Crawling: https://www.theguardian.com/tv-and-radio\n",
      "Crawling: https://www.theguardian.com/stage\n",
      "Crawling: https://www.theguardian.com/music/classicalmusicandopera\n",
      "Crawling: https://www.theguardian.com/games\n",
      "Crawling: https://www.theguardian.com/fashion\n",
      "Crawling: https://www.theguardian.com/food\n",
      "Crawling: https://www.theguardian.com/tone/recipes\n",
      "Crawling: https://www.theguardian.com/lifeandstyle/love-and-sex\n",
      "Crawling: https://www.theguardian.com/lifeandstyle/home-and-garden\n",
      "Crawling: https://www.theguardian.com/lifeandstyle/health-and-wellbeing\n",
      "Crawling: https://www.theguardian.com/lifeandstyle/family\n",
      "Crawling: https://www.theguardian.com/travel\n",
      "Crawling: https://www.theguardian.com/money\n",
      "Crawling: https://www.theguardian.com/guardian-live-events?INTCMP=live_us_header_dropdown\n",
      "Crawling: https://www.theguardian.com/about\n",
      "Crawling: https://www.theguardian.com/video\n",
      "Crawling: https://www.theguardian.com/podcasts\n",
      "Crawling: https://www.theguardian.com/inpictures\n",
      "Crawling: https://www.theguardian.com/insidetheguardian\n",
      "Crawling: https://www.theguardian.com/weekly?INTCMP=gdnwb_mawns_editorial_gweekly_GW_TopNav_US\n",
      "Crawling: https://www.theguardian.com/crosswords\n",
      "Crawling: https://www.theguardian.com/theguardian/series/corrections-and-clarifications\n",
      "Crawling: https://www.theguardian.com/tips\n",
      "Crawling: https://www.theguardian.com/world/europe-news\n",
      "Crawling: https://www.theguardian.com/world/americas\n",
      "Crawling: https://www.theguardian.com/world/asia\n",
      "Crawling: https://www.theguardian.com/australia-news\n",
      "Crawling: https://www.theguardian.com/world/africa\n",
      "Crawling: https://www.theguardian.com/inequality\n",
      "Crawling: https://www.theguardian.com/global-development\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/ethiopia-inaugurates-africa-largest-hydroelectric-dam-egypt-rift-deepens\n",
      "Saved article (3429 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/joseph-kony-icc-hearing-the-hague-alleged-atrocities\n",
      "Saved article (3379 chars)\n",
      "Crawling: https://www.theguardian.com/environment/2025/sep/08/green-economy-boom-africa-climate-summit-renewable-energy-solar\n",
      "Saved article (6708 chars)\n",
      "Crawling: https://www.theguardian.com/global-development/2025/sep/03/trumps-aid-cuts-in-east-africa-led-to-unwanted-abortions-and-babies-being-born-with-hiv-report\n",
      "Saved article (3846 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/2025/sep/02/lawyers-say-men-deported-by-us-to-eswatini-are-being-imprisoned-illegally\n",
      "Saved article (4068 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/canada-bees-honey-heist\n",
      "Saved article (2065 chars)\n",
      "Crawling: https://www.theguardian.com/global-development/2025/sep/09/peru-accused-of-violating-human-rights-after-government-rejects-yavari-mirim-reserve-for-uncontacted-peoples\n",
      "Saved article (5049 chars)\n",
      "Crawling: https://www.theguardian.com/uk-news/2025/sep/08/boris-johnson-nicolas-maduro-meeting-invoice\n",
      "Saved article (7500 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/08/argentina-election-javier-milei\n",
      "Saved article (3894 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/2025/sep/07/jd-vance-venezuelan-boat-strike-rand-paul\n",
      "Saved article (2804 chars)\n",
      "Crawling: https://www.theguardian.com/world/asia-pacific\n",
      "Crawling: https://www.theguardian.com/australia-news/2025/sep/09/albanese-went-to-vanuatu-to-sign-a-500m-agreement-but-leaves-empty-handed-thanks-to-concerns-about-china\n",
      "Saved article (3885 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/thaksin-shinawatra-former-prime-minister-jailed-by-thailand-supreme-court-for-one-year-ntwnfb\n",
      "Saved article (3871 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/tom-phillips-new-zealand-remote-campsite-images-pictures-revealed\n",
      "Saved article (4434 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/08/tom-phillips-new-zealand-fugitive-father-shot-dead-nz-police\n",
      "Saved article (5453 chars)\n",
      "Crawling: https://www.theguardian.com/world/live/2025/sep/08/tom-phillips-shooting-live-updates-nz-police-search-for-children-of-fugitive-father-shot-new-zealand-police-latest-updates\n",
      "Saved article (7522 chars)\n",
      "Crawling: https://www.theguardian.com/australia-news/2025/sep/10/victorian-labor-targets-affluent-melbourne-suburbs-for-rezoning-to-allow-16-storey-apartment-towers\n",
      "Saved article (3615 chars)\n",
      "Crawling: https://www.theguardian.com/business/2025/sep/10/slashing-migration-would-actually-lead-to-higher-house-prices-in-australia-heres-why\n",
      "Saved article (4136 chars)\n",
      "Crawling: https://www.theguardian.com/australia-news/2025/sep/10/pocock-questions-bishops-anu-leadership-as-pressure-builds-for-chancellor-and-vice-chancellor-to-step-down\n",
      "Saved article (5651 chars)\n",
      "Crawling: https://www.theguardian.com/australia-news/2025/sep/10/proposed-nation-leading-nsw-childcare-reforms-to-include-500000-fines\n",
      "Saved article (3398 chars)\n",
      "Crawling: https://www.theguardian.com/australia-news/2025/sep/09/kerry-stokes-ordered-to-pay-ben-roberts-smiths-135m-legal-costs-after-failed-defamation-suit\n",
      "Saved article (2018 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/von-der-leyen-under-growing-pressure-to-show-leadership-over-israel-gaza-war\n",
      "Saved article (4635 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/francois-bayrou-france-block-everything-protests-macron\n",
      "Saved article (4158 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/pigs-heads-found-mosques-paris-region\n",
      "Saved article (2385 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/spanish-government-moves-ban-smoking-bar-terraces\n",
      "Saved article (3208 chars)\n",
      "Crawling: https://www.theguardian.com/world/live/2025/sep/09/france-crisis-government-emmanuel-macron-europe-latest-live-news-updates\n",
      "Saved article (14102 chars)\n",
      "Crawling: https://www.theguardian.com/world/live/2025/sep/09/israel-gaza-city-evacuation-order-idf-military-offensive-live-updates-middle-east-crisis-latest-news\n",
      "Saved article (10840 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/israel-targets-top-hamas-members-in-qatar-for-gaza-ceasefire-talks\n",
      "Saved article (7122 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/the-gaza-family-torn-apart-by-idf-snipers-from-chicago-and-munich\n",
      "Saved article (12584 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/uk-not-concluded-israel-committing-genocide-in-gaza-lammy-says\n",
      "Saved article (5318 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/flotilla-carrying-aid-to-gaza-struck-by-flaming-object-video-shows-sidi-bou-said-port-tunisia\n",
      "Saved article (3628 chars)\n",
      "Crawling: https://www.theguardian.com/world/south-and-central-asia\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/nepal-protests-social-media-ban-lifted-gen-z-kathmandu\n",
      "Saved article (5602 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/08/nepal-bans-26-social-media-sites-including-x-whatsapp-and-youtube\n",
      "Saved article (5012 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/06/everything-gone-punjabi-farmers-suffer-worst-floods-three-decades\n",
      "Saved article (6253 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/04/afghan-earthquake-death-toll-jumps-says-taliban\n",
      "Saved article (4491 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/05/scared-losing-jobs-industries-india-fear-impact-trump-tariffs\n",
      "Saved article (6186 chars)\n",
      "Crawling: https://www.theguardian.com/uk-news\n",
      "Crawling: https://www.theguardian.com/education/2025/sep/09/school-absence-big-factor-child-mental-illness-england-ons-data\n",
      "Saved article (4700 chars)\n",
      "Crawling: https://www.theguardian.com/politics/2025/sep/09/bridget-phillipson-labour-deputy-leader-race\n",
      "Saved article (6013 chars)\n",
      "Crawling: https://www.theguardian.com/travel/2025/sep/09/man-arrested-suspicion-bringing-teargas-heathrow-airport\n",
      "Saved article (1637 chars)\n",
      "Crawling: https://www.theguardian.com/business/2025/sep/09/ben-jerrys-founders-call-brand-excluded-from-unilever-listing\n",
      "Saved article (3130 chars)\n",
      "Crawling: https://www.theguardian.com/politics/live/2025/sep/09/bridget-phillipson-labour-deputy-leader-uk-politics-latest-news-updates-keir-starmer\n",
      "Saved article (21613 chars)\n",
      "Crawling: https://www.theguardian.com/politics/live/2025/sep/09/bridget-phillipson-labour-deputy-leader-uk-politics-latest-news-updates-keir-starmer#comments\n",
      "Saved article (21613 chars)\n",
      "Crawling: https://www.theguardian.com/uk-news/all\n",
      "Crawling: https://www.theguardian.com/us\n",
      "Crawling: https://www.theguardian.com/us-news/live/2025/sep/09/donald-trump-jeffrey-epstein-birthday-note-letter-latest-live-us-politics-news\n",
      "Saved article (14990 chars)\n",
      "Crawling: https://www.theguardian.com/technology/2025/sep/09/apple-iphone-17\n",
      "Saved article (2934 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/2025/sep/09/epstein-50th-birthday-book-who-is-in-it-and-what-did-they-say-donald-trump-bill-clinton-peter-mandelson\n",
      "Saved article (6262 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/2025/sep/09/trump-epstein-photo-check-woman\n",
      "Saved article (3770 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/2025/sep/09/trump-jeffrey-epstein-birthday-note\n",
      "Saved article (3124 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/all\n",
      "Crawling: https://www.theguardian.com/tabs-popular-0\n",
      "Failed to fetch https://www.theguardian.com/tabs-popular-0: 404 Client Error: Not Found for url: https://www.theguardian.com/tabs-popular-0\n",
      "Crawling: https://www.theguardian.com/tabs-popular-1\n",
      "Failed to fetch https://www.theguardian.com/tabs-popular-1: 404 Client Error: Not Found for url: https://www.theguardian.com/tabs-popular-1\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/09/pro-palestine-activists-call-for-arrest-israeli-president-isaac-herzog-during-uk-visit\n",
      "Saved article (3624 chars)\n",
      "Crawling: https://www.theguardian.com/world/2025/sep/08/doorbell-prankster-that-tormented-residents-of-german-apartments-turns-out-to-be-a-slug\n",
      "Saved article (1949 chars)\n",
      "Crawling: https://www.theguardian.com/us-news/gallery/2025/sep/09/jeffrey-epstein-birthday-book-in-pictures\n",
      "Saved article (418 chars)\n",
      "Crawling: https://www.theguardian.com/sport/2025/sep/08/caitlin-clark-wnba-season-injury\n",
      "Saved article (3613 chars)\n",
      "Crawling: https://www.theguardian.com/culture/2025/sep/09/jon-stewart-trump-health-epstein\n",
      "Saved article (6829 chars)\n",
      "Crawling: https://www.theguardian.com/environment/2025/sep/08/disposable-face-masks-covid-chemical-timebomb\n",
      "Saved article (2973 chars)\n",
      "Crawling: https://www.theguardian.com/politics/2025/sep/09/peter-mandelson-called-jeffrey-epstein-my-best-pal-in-50th-birthday-letter\n",
      "Saved article (3336 chars)\n",
      "Crawling: https://www.theguardian.com/environment/2025/sep/09/against-the-tide-tendring-essex-youth-jobs-opportunities-seaside-deprivation-arts-music-theatre\n",
      "Saved article (7966 chars)\n",
      "Crawling: https://www.theguardian.com/film/2025/sep/09/israeli-film-industry-calls-boycott-pledge-deeply-troubling\n",
      "Saved article (10157 chars)\n",
      "Crawling: https://www.theguardian.com/world/israel\n",
      "Crawling: https://www.theguardian.com/world/gaza\n",
      "Crawling: https://www.theguardian.com/world/palestinian-territories\n",
      "Crawling: https://www.theguardian.com/info/2018/sep/17/guardian-us-morning-briefing-sign-up-to-stay-informed\n",
      "Crawling: https://www.theguardian.com/info/about-guardian-us\n",
      "Crawling: https://www.theguardian.com/info/complaints-and-corrections\n",
      "Crawling: https://www.theguardian.com/securedrop\n",
      "Crawling: https://www.theguardian.com/info/privacy\n",
      "Crawling: https://www.theguardian.com/info/cookies\n",
      "Crawling: https://www.theguardian.com/help/terms-of-service\n",
      "Crawling: https://www.theguardian.com/info/about-guardian-us/contact\n",
      "Crawling: https://www.theguardian.com/index/subjects/a\n",
      "Crawling: https://www.theguardian.com/email-newsletters?INTCMP=DOTCOM_FOOTER_NEWSLETTER_US\n",
      "Crawling: https://www.theguardian.com/guardian-labs-us\n",
      "Crawling: https://www.theguardian.com/help/accessibility-help\n",
      "Crawling: https://www.theguardian.com/world#top\n",
      "Collected 55 articles\n"
     ]
    }
   ],
   "source": [
    "def extract_text(html, url):\n",
    "    \"\"\"Try to extract main article text using trafilatura, fallback to <p> tags.\"\"\"\n",
    "    text = trafilatura.extract(html, url=url)\n",
    "    if text:\n",
    "        return text\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
    "\n",
    "def crawl(url, index, depth=1):\n",
    "    if depth == 0 or url in visited:\n",
    "        return\n",
    "    \n",
    "    if True:\n",
    "        print(f\"Crawling: {url}\")\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return\n",
    "        \n",
    "        html = response.text\n",
    "        \n",
    "        text = extract_text(html, url)\n",
    "        \n",
    "        if text:\n",
    "            if \"2025\" in url:\n",
    "                articles.append({\"url\": url, \"id\": 0, \"title\": 0, \"writer\": 0,  \"raw_text\": text})\n",
    "                print(f\"Saved article ({len(text)} chars)\")\n",
    "        else:\n",
    "            print(f\"Not an article, but checking links...\")\n",
    "        \n",
    "        # Parse links\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            new_url = urljoin(url, link[\"href\"])\n",
    "            \n",
    "            # Filter: must be within same domain as seed\n",
    "            if new_url not in visited and url.split(\"/\")[2] in new_url:\n",
    "                crawl(new_url, index, depth - 1)\n",
    "\n",
    "\n",
    "crawl(news_seeds[0], 0, depth=2)\n",
    "\n",
    "print(f\"Collected {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The federal government is racing to save a major new agreement with Vanuatu, after Anthony Albanese’s plans to sign the deal were rebuffed over concerns about infrastructure funding from China.\n",
      "Speaking alongside Vanuatu’s prime minister, Jotham Napat, on Tuesday, Albanese said he was confident the Nakamal agreement will be “able to be signed soon”, talking up cooperation and proper process with Vanuatu’s governing coalition.\n",
      "The Australian prime minister travelled to Port Vila before this week’s Pacific Islands Forum, hoping to sign the agreement, which would see Australia spend up to $500m over a decade on climate change resilience, security services and the economy.\n",
      "Sign up: AU Breaking News email\n",
      "It is designed to lock in Australia as Vanuatu’s primary security partner and to push back against China’s efforts to build ties across the Pacific, including through funding for major infrastructure projects.\n",
      "“It’s important that processes be respected and I respect the prime minister’s processes,” Albanese said.\n",
      "“We have our own, respectively. This is an agreement that will also, importantly, respect sovereignty of Vanuatu, but one as well that respects sovereignty of Australia.”\n",
      "But Napat said more work was needed before the deal could be signed.\n",
      "“Some of my ministers, and my MPs, they feel that it requires more discussions to particularly on some of the specific wordings in the agreement … when it comes to the critical infrastructure.”\n",
      "Asked if the concerns related to Vanuatu’s ability to seek infrastructure funding from other countries, Napat said: “Yes.”\n",
      "Last month, senior Australian ministers including Penny Wong, Richard Marles and Pat Conroy visited Vanuatu to celebrate the conclusion of negotiations.\n",
      "Albanese said there was in-principle agreement but both sides had more work to do. He stopped short of a specific timeframe, saying only he was confident the agreement would be completed “soon”.\n",
      "A previous security agreement struck with Vanuatu in 2022 was never formally ratified by the country’s parliament, amid concerns over a lack of consultation and whether it was consistent with Vanuatu’s philosophy of non-alignment in foreign policy.\n",
      "“I’m reassured by the discussion that I’ve had with the prime minister,” Albanese said.\n",
      "“Australia was a great supporter of independence with Vanuatu. We respect its processes, we respect its sovereignty, which is why we respect the discussion that we’ve had together today.\n",
      "“If that means people going through processes for a short period of time, then that is absolutely fine by us, and we don’t want to either do or be seen for anything to occur that undermines the sovereignty of Vanuatu.”\n",
      "Nakamal is the Bislama word for a traditional meeting place.\n",
      "China has funded road projects in Vanuatu, as well as building new government ministry buildings, the country’s parliament, a sports stadium, a convention centre and a wharf.\n",
      "To counter China’s influence, Australia is spending $200m to train Pacific police recruits in Brisbane, and signing agreements with countries including Papua New Guinea and Solomon Islands.\n",
      "Albanese will fly with Napat to Honiara on Wednesday for the start of the Pacific Islands Forum.\n",
      "China’s presence in the region risks overshadowing that event too. The lead-up to the talks has already seen significant tensions after Solomon Islands’ prime minister, Jeremiah Manele, excluded external partners – including China, the US and Taiwan – from the event.\n",
      "Solomon Islands is China’s biggest security ally in the Pacific and Manele’s decision to block external partners fuelled speculation he was trying to keep Taiwan out.\n",
      "Manele said the exclusion of partners was justified because a review of regional architecture in the Pacific was not yet completed.\n",
      "The forum is expected to endorse a Pacific resilience facility and a declaration calling the Pacific an “Ocean of Peace”.\n"
     ]
    }
   ],
   "source": [
    "print(articles[10]['raw_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/AmirhosseinRaeghi-400522373/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/AmirhosseinRaeghi-400522373/nltk_data...\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/AmirhosseinRaeghi-400522373/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      2\u001b[39m stemmer = PorterStemmer()\n\u001b[32m      3\u001b[39m lemmatizer = WordNetLemmatizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(articles):\n",
    "    for article in articles:\n",
    "        text = article['raw_text']\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        # Remove special characters (keep only words and numbers)\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "        article['normalize_text'] = text\n",
    "\n",
    "# normalize(articles)\n",
    "# print(articles[10]['normalize_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(articles):\n",
    "    for article in articles:\n",
    "        tokens = nltk.word_tokenize(article['normalize_text'])\n",
    "    \n",
    "        # Remove stopwords\n",
    "        article[\"tokens\"] = [t for t in tokens if t not in stop_words] \n",
    "\n",
    "# tokenize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [stemmer.stem(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# stem(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [lemmatizer.lemmatize(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# lemmatize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     stem(articles)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# lemmatize(articles)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(articles)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(articles):\n\u001b[32m      2\u001b[39m     normalize(articles)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     stem(articles)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(articles)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(articles):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalize_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m      6\u001b[39m         article[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def preprocess(articles):\n",
    "    normalize(articles)\n",
    "    tokenize(articles)\n",
    "    stem(articles)\n",
    "    # lemmatize(articles)\n",
    "    \n",
    "preprocess(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tin'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = 200000\n",
    "\n",
    "dataset_articles = []\n",
    "for fileid in reuters.fileids()[:dataset_size]:\n",
    "    text = reuters.raw(fileid)\n",
    "    cats = reuters.categories(fileid)  # some docs have multiple categories\n",
    "    if cats:\n",
    "        dataset_articles.append({\"raw_text\": text, \"label\": cats[0]})\n",
    "\n",
    "dataset_articles[10][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_articles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(articles)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(articles):\n\u001b[32m      2\u001b[39m     normalize(articles)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     stem(articles)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(articles)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(articles):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnormalize_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m      6\u001b[39m         article[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/AmirhosseinRaeghi-400522373/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/share/nltk_data'\n    - '/home/AmirhosseinRaeghi-400522373/Amirhossein life/Uni/term8/webSearch/Project/Web-News-Classifier/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "preprocess(dataset_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X_reuters = vectorizer.fit_transform([\" \".join(dataset_article[\"tokens\"]) for dataset_article in dataset_articles])\n",
    "X_crawled = vectorizer.transform([\" \".join(article[\"tokens\"]) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vocab = {}\n",
    "labels = [dataset_article[\"label\"] for dataset_article in dataset_articles]\n",
    "labels = list(set(labels))  # unique labels\n",
    "\n",
    "for label in labels:\n",
    "    # find indices of docs belonging to this label\n",
    "    idx = [i for i, art in enumerate(dataset_articles) if art[\"label\"] == label]\n",
    "    \n",
    "    if not idx:  # skip empty categories\n",
    "        continue\n",
    "    \n",
    "    # average TF-IDF weights for this category\n",
    "    avg_weights = np.asarray(X_reuters[idx].mean(axis=0)).ravel()\n",
    "    \n",
    "    # get indices of top 50 highest-weighted terms\n",
    "    top_idx = avg_weights.argsort()[::-1][:50]\n",
    "    \n",
    "    # get actual term strings\n",
    "    terms = vectorizer.get_feature_names_out()[top_idx]\n",
    "    \n",
    "    # get their corresponding weights\n",
    "    weights = avg_weights[top_idx]\n",
    "    \n",
    "    # save into dictionary\n",
    "    category_vocab[label] = dict(zip(terms, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_article(article_vector):\n",
    "    scores = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    vec_array = article_vector.toarray().ravel()\n",
    "    for label, vocab in category_vocab.items():\n",
    "        score = sum(vec_array[feature_names.tolist().index(term)] * w\n",
    "                    for term, w in vocab.items() if term in feature_names)\n",
    "        scores[label] = score\n",
    "    return scores\n",
    "\n",
    "for article, x in zip(articles, X_crawled):\n",
    "    scores = score_article(x)  # returns a dict of {label: score}\n",
    "    article[\"scores\"] = scores\n",
    "    article[\"predicted\"] = max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[10][\"predicted\"]\n",
    "print(Counter(a[\"predicted\"] for a in articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:5]:\n",
    "    print(article[\"raw_text\"])\n",
    "    print(f\"### predicted class: {article['predicted']}\")\n",
    "    print(\"*\"*20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
