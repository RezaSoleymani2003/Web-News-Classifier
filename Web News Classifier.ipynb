{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time\n",
    "import time\n",
    "\n",
    "#Random\n",
    "import random\n",
    "\n",
    "#CSV\n",
    "import csv  \n",
    "\n",
    "#OS\n",
    "import os\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import trafilatura\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init -> (userAgent)(no need)\n",
    "→ (login)(Optional)\n",
    "→ loop { \n",
    "     rateLimiter\n",
    "     randomDelay\n",
    "     fetch/searchSubReddit\n",
    "     cleanData(deduplication + canonicalization)\n",
    "     saveData(incremental)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "def init():\n",
    "    try:\n",
    "        service = Service(\"/usr/bin/geckodriver\")\n",
    "        driver = webdriver.Firefox(service=service)\n",
    "        return driver\n",
    "    except WebDriverException as e:\n",
    "        print(\"WebDriver error happened:\", e)\n",
    "        return None\n",
    "    finally:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomDelay\n",
    "def randomDelay():\n",
    "    time.sleep(random.uniform(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rateLimiter\n",
    "def rate_limiter(searchSubReddit,totalPost, posts_before_cooldown=30, cooldown_time=60):\n",
    "    counter = 0\n",
    "    for _ in range(totalPost):\n",
    "        searchSubReddit()\n",
    "        counter += 1\n",
    "\n",
    "        randomDelay()\n",
    "\n",
    "        if counter >= posts_before_cooldown:\n",
    "            print(\"Cooldown Between 30 Topics\")\n",
    "            time.sleep(cooldown_time)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subReddits List\n",
    "subRedditsNews = [\n",
    "    \"https://www.reddit.com/r/news/\",\n",
    "    \"https://www.reddit.com/r/worldnews/\",\n",
    "    \"https://www.reddit.com/r/realbbcnews/\",\n",
    "    \"https://www.reddit.com/r/world24x7hr/\"\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch/searchSubReddit\n",
    "def searchRedditNews(subRedditsNews):\n",
    "    driver = init()\n",
    "    for item in range(len(subRedditsNews)):        \n",
    "        driver.get(subRedditsNews[item])\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        \n",
    "        randomDelay()\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchRedditNews(subRedditsNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_seeds = [\n",
    "    \"https://www.theguardian.com/world\", # 54\n",
    "    \"https://www.aljazeera.com/news/\", # 19\n",
    "    \"https://www.nytimes.com/section/world\",# forbidden url\n",
    "    \"https://www.nbcnews.com/world\",# keyword: world , 22\n",
    "]\n",
    "\n",
    "visited = set()\n",
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html, url):\n",
    "    \"\"\"Try to extract main article text using trafilatura, fallback to <p> tags.\"\"\"\n",
    "    text = trafilatura.extract(html, url=url)\n",
    "    if text:\n",
    "        return text\n",
    "    else:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
    "\n",
    "def crawl(url, index, depth=1):\n",
    "    if depth == 0 or url in visited:\n",
    "        return\n",
    "    \n",
    "    if True:\n",
    "        print(f\"Crawling: {url}\")\n",
    "        visited.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            return\n",
    "        \n",
    "        html = response.text\n",
    "        \n",
    "        text = extract_text(html, url)\n",
    "        \n",
    "        if text:\n",
    "            if \"2025\" in url:\n",
    "                articles.append({\"url\": url, \"id\": 0, \"title\": 0, \"writer\": 0,  \"raw_text\": text})\n",
    "                print(f\"Saved article ({len(text)} chars)\")\n",
    "        else:\n",
    "            print(f\"Not an article, but checking links...\")\n",
    "        \n",
    "        # Parse links\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            new_url = urljoin(url, link[\"href\"])\n",
    "            \n",
    "            # Filter: must be within same domain as seed\n",
    "            if new_url not in visited and url.split(\"/\")[2] in new_url:\n",
    "                crawl(new_url, index, depth - 1)\n",
    "\n",
    "\n",
    "crawl(news_seeds[0], 0, depth=2)\n",
    "\n",
    "print(f\"Collected {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[10]['raw_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCsv(\n",
    "    type,\n",
    "    source,\n",
    "    url,\n",
    "    id,\n",
    "    topic,\n",
    "    author,\n",
    "    timeRelease,\n",
    "    vote,\n",
    "    commentsCount,\n",
    "    language,\n",
    "    token,\n",
    "    predicte,\n",
    "):\n",
    "    columnTopics = [\n",
    "        \"Type\",\n",
    "        \"Source\",\n",
    "        \"URL\",\n",
    "        \"ID\",\n",
    "        \"Topic\",\n",
    "        \"Author\",\n",
    "        \"TimeRelease\",\n",
    "        \"Vote\",\n",
    "        \"CommentsCount\",\n",
    "        \"Language\",\n",
    "        \"Token\",\n",
    "        \"Predicte\",\n",
    "    ]\n",
    "    row = [\n",
    "        type,\n",
    "        source,\n",
    "        url,\n",
    "        id,\n",
    "        topic,\n",
    "        author,\n",
    "        timeRelease,\n",
    "        vote,\n",
    "        commentsCount,\n",
    "        language,\n",
    "        token,\n",
    "        predicte,\n",
    "    ]\n",
    "    \n",
    "    fileName = \"RecordsNewsReddit.csv\"\n",
    "    file_exists = os.path.exists(fileName)\n",
    "    \n",
    "    with open(fileName, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(columnTopics)\n",
    "        \n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Save Data\n",
    "saveCsv(\n",
    "    type=\"Reddit\",\n",
    "    source=\"r/news\",\n",
    "    url=\"https://reddit.com/r/news/xyz\",\n",
    "    id=\"abc123\",\n",
    "    topic=\"Breaking News\",\n",
    "    author=\"user42\",\n",
    "    timeRelease=\"2025-09-08 12:00\",\n",
    "    vote=420,\n",
    "    commentsCount=69,\n",
    "    language=\"en\",\n",
    "    token=\"breaking,news\",\n",
    "    predicte=\"Politics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(articles):\n",
    "    for article in articles:\n",
    "        text = article['raw_text']\n",
    "        text = text.lower()\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        # Remove special characters (keep only words and numbers)\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "        article['normalize_text'] = text\n",
    "\n",
    "# normalize(articles)\n",
    "# print(articles[10]['normalize_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(articles):\n",
    "    for article in articles:\n",
    "        tokens = nltk.word_tokenize(article['normalize_text'])\n",
    "    \n",
    "        # Remove stopwords\n",
    "        article[\"tokens\"] = [t for t in tokens if t not in stop_words] \n",
    "\n",
    "# tokenize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [stemmer.stem(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# stem(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(articles):\n",
    "    for article in articles:\n",
    "        article[\"tokens\"] = [lemmatizer.lemmatize(t) for t in article[\"tokens\"]]\n",
    "\n",
    "# lemmatize(articles)\n",
    "# print(articles[10]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(articles):\n",
    "    normalize(articles)\n",
    "    tokenize(articles)\n",
    "    stem(articles)\n",
    "    # lemmatize(articles)\n",
    "    \n",
    "preprocess(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 200000\n",
    "\n",
    "dataset_articles = []\n",
    "for fileid in reuters.fileids()[:dataset_size]:\n",
    "    text = reuters.raw(fileid)\n",
    "    cats = reuters.categories(fileid)  # some docs have multiple categories\n",
    "    if cats:\n",
    "        dataset_articles.append({\"raw_text\": text, \"label\": cats[0]})\n",
    "\n",
    "dataset_articles[10][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(dataset_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X_reuters = vectorizer.fit_transform([\" \".join(dataset_article[\"tokens\"]) for dataset_article in dataset_articles])\n",
    "X_crawled = vectorizer.transform([\" \".join(article[\"tokens\"]) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vocab = {}\n",
    "labels = [dataset_article[\"label\"] for dataset_article in dataset_articles]\n",
    "labels = list(set(labels))  # unique labels\n",
    "\n",
    "for label in labels:\n",
    "    # find indices of docs belonging to this label\n",
    "    idx = [i for i, art in enumerate(dataset_articles) if art[\"label\"] == label]\n",
    "    \n",
    "    if not idx:  # skip empty categories\n",
    "        continue\n",
    "    \n",
    "    # average TF-IDF weights for this category\n",
    "    avg_weights = np.asarray(X_reuters[idx].mean(axis=0)).ravel()\n",
    "    \n",
    "    # get indices of top 50 highest-weighted terms\n",
    "    top_idx = avg_weights.argsort()[::-1][:50]\n",
    "    \n",
    "    # get actual term strings\n",
    "    terms = vectorizer.get_feature_names_out()[top_idx]\n",
    "    \n",
    "    # get their corresponding weights\n",
    "    weights = avg_weights[top_idx]\n",
    "    \n",
    "    # save into dictionary\n",
    "    category_vocab[label] = dict(zip(terms, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_article(article_vector):\n",
    "    scores = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    vec_array = article_vector.toarray().ravel()\n",
    "    for label, vocab in category_vocab.items():\n",
    "        score = sum(vec_array[feature_names.tolist().index(term)] * w\n",
    "                    for term, w in vocab.items() if term in feature_names)\n",
    "        scores[label] = score\n",
    "    return scores\n",
    "\n",
    "for article, x in zip(articles, X_crawled):\n",
    "    scores = score_article(x)  # returns a dict of {label: score}\n",
    "    article[\"scores\"] = scores\n",
    "    article[\"predicted\"] = max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[10][\"predicted\"]\n",
    "print(Counter(a[\"predicted\"] for a in articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:5]:\n",
    "    print(article[\"raw_text\"])\n",
    "    print(f\"### predicted class: {article['predicted']}\")\n",
    "    print(\"*\"*20)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
